# RAG System Configuration for Air Conditioners (Amazon Products)

# Ollama configuration
ollama:
  base_url: "http://localhost:11434"
  timeout: 120

# Dataset configuration
dataset:
  name: "amazon_products"
  path: "data/Air Conditioners.csv"
  cache_dir: "data/cache"

# Embedding model configuration
embedding:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384
  batch_size: 32
  device: "cpu"  # Using CPU due to CUDA compatibility issues

# Vector database configuration
vector_db:
  type: "faiss"  # or "chromadb"
  index_path: "data/vector_index"
  similarity_metric: "cosine"

# LLM configurations - Using Ollama for local models
llms:
  phi3:
    type: "ollama"
    model_name: "phi3"
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.9

  llama3:
    type: "ollama"
    model_name: "llama3"
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.9

  gemma2:
    type: "ollama"
    model_name: "gemma2"
    max_new_tokens: 512
    temperature: 0.7
    top_p: 0.9


# RAG configuration
rag:
  top_k: 3
  chunk_size: 512
  chunk_overlap: 50
  reranking: false

# Evaluation configuration
evaluation:
  metrics:
    - "answer_relevancy"
    - "faithfulness"
    - "context_precision"
    - "response_length"
  output_dir: "outputs"

# Generation prompts
prompts:
  system_prompt: |
    You are a helpful AI assistant specialized in answering questions about Amazon products.
    Use the provided context to answer the user's question accurately and concisely.
    If you cannot find the answer in the context, say so clearly.

  qa_template: |
    Context information is below:
    ---------------------
    {context}
    ---------------------

    Given the context information and not prior knowledge, answer the query.
    Query: {query}
    Answer:
