{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Conditioner RAG System with Ollama\n",
    "## ShopSmart E-commerce Intelligence System\n",
    "\n",
    "This notebook demonstrates the RAG system using local Ollama models:\n",
    "- Phi-3\n",
    "- Llama3  \n",
    "- Gemma2\n",
    "\n",
    "**Dataset**: Air Conditioners from Amazon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from src.data_loader import AmazonProductDataLoader\n",
    "from src.vector_store import VectorStore\n",
    "from src.ollama_handler import OllamaMultiLLMManager\n",
    "from src.rag_system import RAGSystem\n",
    "from src.evaluation import RAGEvaluator\n",
    "from src.questions import get_all_questions\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Dataset path: {config['dataset']['path']}\")\n",
    "print(f\"Embedding model: {config['embedding']['model_name']}\")\n",
    "print(f\"Ollama URL: {config['ollama']['base_url']}\")\n",
    "print(f\"\\nConfigured LLMs: {list(config['llms'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Air Conditioner Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_loader = AmazonProductDataLoader(config['dataset']['path'])\n",
    "df = data_loader.load_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "stats = data_loader.get_statistics()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total products: {stats['total_products']}\")\n",
    "print(f\"\\nColumns: {stats['columns']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price distribution\n",
    "import re\n",
    "\n",
    "def parse_price(price_str):\n",
    "    if pd.isna(price_str):\n",
    "        return 0\n",
    "    cleaned = re.sub(r'[^\\d.]', '', str(price_str))\n",
    "    return float(cleaned) if cleaned else 0\n",
    "\n",
    "df['price_numeric'] = df['discount_price'].apply(parse_price)\n",
    "df_clean = df[df['price_numeric'] > 0]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_clean['price_numeric'], bins=30, edgecolor='black')\n",
    "plt.xlabel('Price (₹)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Price Distribution of Air Conditioners')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(df_clean['price_numeric'])\n",
    "plt.ylabel('Price (₹)')\n",
    "plt.title('Price Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Price range: ₹{df_clean['price_numeric'].min():.2f} - ₹{df_clean['price_numeric'].max():.2f}\")\n",
    "print(f\"Average price: ₹{df_clean['price_numeric'].mean():.2f}\")\n",
    "print(f\"Median price: ₹{df_clean['price_numeric'].median():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df = data_loader.preprocess_data()\n",
    "documents = data_loader.create_documents()\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(documents[0]['text'][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vector_store = VectorStore(\n",
    "    embedding_model_name=config['embedding']['model_name'],\n",
    "    index_path=config['vector_db']['index_path'],\n",
    "    use_faiss=True\n",
    ")\n",
    "\n",
    "# Create embeddings\n",
    "print(\"Creating embeddings... (this may take a minute)\")\n",
    "embeddings = vector_store.create_embeddings(documents, batch_size=32)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and save index\n",
    "vector_store.build_index()\n",
    "print(\"Vector index built!\")\n",
    "\n",
    "vector_store.save_index()\n",
    "print(\"Index saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"Best budget 1.5 ton inverter AC with good energy rating\"\n",
    "\n",
    "results = vector_store.search(test_query, top_k=5)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Top 5 Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   {result['document']['text'][:250]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Ollama LLMs\n",
    "\n",
    "**Important**: Make sure Ollama is running and you have downloaded the models:\n",
    "```bash\n",
    "ollama pull phi3\n",
    "ollama pull llama3\n",
    "ollama pull gemma2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Ollama LLM Manager\n",
    "llm_manager = OllamaMultiLLMManager(\n",
    "    llm_configs=config['llms'],\n",
    "    base_url=config['ollama']['base_url']\n",
    ")\n",
    "\n",
    "print(\"Ollama LLM Manager initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one model for testing (Phi-3 is fastest/smallest)\n",
    "try:\n",
    "    llm_manager.load_llm('phi3')\n",
    "    print(\"✓ Phi-3 loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Phi-3: {e}\")\n",
    "    print(\"\\nMake sure:\")\n",
    "    print(\"1. Ollama is running: ollama serve\")\n",
    "    print(\"2. Model is pulled: ollama pull phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = RAGSystem(\n",
    "    vector_store=vector_store,\n",
    "    llm_manager=llm_manager,\n",
    "    top_k=config['rag']['top_k'],\n",
    "    system_prompt=config['prompts']['system_prompt'],\n",
    "    qa_template=config['prompts']['qa_template']\n",
    ")\n",
    "\n",
    "print(\"RAG system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test RAG System with Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question\n",
    "question = \"What are the best value air conditioners under ₹35,000?\"\n",
    "\n",
    "result = rag_system.query(\n",
    "    query=question,\n",
    "    llm_name='phi3',\n",
    "    return_context=True\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"Answer (Phi-3):\")\n",
    "print(\"=\"*80)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View retrieved context\n",
    "print(\"Retrieved Documents:\\n\")\n",
    "for i, doc in enumerate(result['retrieved_docs']):\n",
    "    print(f\"{i+1}. Score: {doc['score']:.3f}\")\n",
    "    print(f\"   {doc['document']['text'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare All Three Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models\n",
    "print(\"Loading all models...\\n\")\n",
    "\n",
    "for model_name in ['phi3', 'llama3', 'gemma2']:\n",
    "    try:\n",
    "        if model_name not in llm_manager.get_loaded_llms():\n",
    "            llm_manager.load_llm(model_name)\n",
    "            print(f\"✓ {model_name} loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {model_name}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded models: {llm_manager.get_loaded_llms()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test same question with all models\n",
    "test_question = \"Which 1.5 ton inverter AC offers the best value under ₹40,000?\"\n",
    "\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_all = rag_system.query_all_llms(\n",
    "    query=test_question,\n",
    "    return_context=False\n",
    ")\n",
    "\n",
    "for model_name, result in results_all.items():\n",
    "    print(f\"\\n{model_name.upper()} Response:\")\n",
    "    print(\"-\"*80)\n",
    "    print(result['answer'])\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test with Evaluation Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation questions\n",
    "questions = get_all_questions()\n",
    "\n",
    "print(f\"Total questions: {len(questions)}\\n\")\n",
    "\n",
    "# Display first 5 questions\n",
    "for q in questions[:5]:\n",
    "    print(f\"Q{q['id']}. {q['question']}\")\n",
    "    print(f\"   Category: {q['category']} | Difficulty: {q['difficulty']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test first 3 questions with one model\n",
    "results_list = []\n",
    "\n",
    "for q in questions[:3]:\n",
    "    print(f\"\\nProcessing Q{q['id']}: {q['question']}\")\n",
    "    \n",
    "    result = rag_system.query(\n",
    "        query=q['question'],\n",
    "        llm_name='phi3'\n",
    "    )\n",
    "    \n",
    "    results_list.append({\n",
    "        'question_id': q['id'],\n",
    "        'question': q['question'],\n",
    "        'category': q['category'],\n",
    "        'answer': result['answer'],\n",
    "        'num_retrieved': result['num_retrieved']\n",
    "    })\n",
    "    \n",
    "    print(f\"Answer: {result['answer'][:200]}...\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models on a specific question\n",
    "comparison_question = questions[4]['question']  # Value reasoning question\n",
    "\n",
    "print(f\"Question: {comparison_question}\\n\")\n",
    "\n",
    "comparison_results = {}\n",
    "import time\n",
    "\n",
    "for model_name in llm_manager.get_loaded_llms():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = rag_system.query(\n",
    "        query=comparison_question,\n",
    "        llm_name=model_name\n",
    "    )\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    comparison_results[model_name] = {\n",
    "        'answer': result['answer'],\n",
    "        'time': elapsed_time,\n",
    "        'length': len(result['answer'].split())\n",
    "    }\n",
    "\n",
    "# Display comparison\n",
    "for model_name, data in comparison_results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"Time: {data['time']:.2f}s | Words: {data['length']}\")\n",
    "    print(\"-\"*80)\n",
    "    print(data['answer'])\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison metrics\n",
    "import numpy as np\n",
    "\n",
    "models = list(comparison_results.keys())\n",
    "times = [comparison_results[m]['time'] for m in models]\n",
    "lengths = [comparison_results[m]['length'] for m in models]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Response time\n",
    "ax1.bar(models, times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax1.set_ylabel('Response Time (seconds)')\n",
    "ax1.set_title('Model Response Time Comparison')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Answer length\n",
    "ax2.bar(models, lengths, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax2.set_ylabel('Answer Length (words)')\n",
    "ax2.set_title('Answer Length Comparison')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Interactive Query Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question, model='phi3', show_context=True):\n",
    "    \"\"\"Interactive query function\"\"\"\n",
    "    result = rag_system.query(\n",
    "        query=question,\n",
    "        llm_name=model,\n",
    "        return_context=show_context\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nA ({model}): {result['answer']}\\n\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Retrieved {result['num_retrieved']} documents\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if show_context:\n",
    "        print(\"\\nTop Retrieved Documents:\")\n",
    "        for i, doc in enumerate(result['retrieved_docs'][:3]):\n",
    "            print(f\"\\n{i+1}. (Score: {doc['score']:.3f})\")\n",
    "            print(f\"   {doc['document']['text'][:200]}...\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try it\n",
    "ask_question(\"What is the cheapest 5-star rated AC?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results to CSV\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"../outputs/model_comparison_{timestamp}.json\"\n",
    "\n",
    "# Create outputs directory if needed\n",
    "import os\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(comparison_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Run Full Evaluation**: Test all 15 questions with all 3 models\n",
    "2. **Analyze Results**: Compare accuracy, factuality, and reasoning\n",
    "3. **Generate Report**: Create visualizations and insights\n",
    "4. **Optimize Prompts**: Improve system and QA prompts for better results\n",
    "\n",
    "Use the `main.py` script for automated evaluation:\n",
    "```bash\n",
    "python main.py --evaluate-all\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
