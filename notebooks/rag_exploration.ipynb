{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Product RAG System - Interactive Exploration\n",
    "\n",
    "This notebook provides an interactive way to explore the RAG system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_loader import AmazonProductDataLoader\n",
    "from src.vector_store import VectorStore\n",
    "from src.llm_handler import MultiLLMManager\n",
    "from src.rag_system import RAGSystem\n",
    "from src.evaluation import RAGEvaluator\n",
    "from src.questions import get_all_questions\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Dataset path: {config['dataset']['path']}\")\n",
    "print(f\"Embedding model: {config['embedding']['model_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_loader = AmazonProductDataLoader(config['dataset']['path'])\n",
    "df = data_loader.load_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "stats = data_loader.get_statistics()\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total products: {stats['total_products']}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "for col, count in stats['missing_values'].items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize category distribution if available\n",
    "if 'category' in df.columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df['category'].value_counts().head(15).plot(kind='barh')\n",
    "    plt.title('Top 15 Product Categories')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "df = data_loader.preprocess_data()\n",
    "documents = data_loader.create_documents()\n",
    "\n",
    "print(f\"Created {len(documents)} documents\")\n",
    "print(f\"\\nSample document:\")\n",
    "print(documents[0]['text'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vector_store = VectorStore(\n",
    "    embedding_model_name=config['embedding']['model_name'],\n",
    "    index_path=config['vector_db']['index_path'],\n",
    "    use_faiss=True\n",
    ")\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = vector_store.create_embeddings(documents, batch_size=32)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index\n",
    "vector_store.build_index()\n",
    "print(\"Vector index built!\")\n",
    "\n",
    "# Save index\n",
    "vector_store.save_index()\n",
    "print(\"Index saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query\n",
    "test_query = \"Best wireless headphones with noise cancellation\"\n",
    "\n",
    "results = vector_store.search(test_query, top_k=5)\n",
    "\n",
    "print(f\"Query: {test_query}\\n\")\n",
    "print(\"Top 5 Results:\\n\")\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"{i+1}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   {result['document']['text'][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load LLMs (Choose One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a single LLM for testing (Phi-3 is smallest/fastest)\n",
    "llm_configs = {\n",
    "    'phi3': config['llms']['phi3']\n",
    "}\n",
    "\n",
    "llm_manager = MultiLLMManager(llm_configs)\n",
    "llm_manager.load_llm('phi3')\n",
    "\n",
    "print(\"LLM loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system = RAGSystem(\n",
    "    vector_store=vector_store,\n",
    "    llm_manager=llm_manager,\n",
    "    top_k=config['rag']['top_k'],\n",
    "    system_prompt=config['prompts']['system_prompt'],\n",
    "    qa_template=config['prompts']['qa_template']\n",
    ")\n",
    "\n",
    "print(\"RAG system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test question\n",
    "question = \"What are the best budget-friendly smartphones under $300?\"\n",
    "\n",
    "result = rag_system.query(\n",
    "    query=question,\n",
    "    llm_name='phi3',\n",
    "    return_context=True\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"Answer:\")\n",
    "print(\"=\"*80)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View retrieved context\n",
    "print(\"Retrieved Documents:\\n\")\n",
    "for i, doc in enumerate(result['retrieved_docs']):\n",
    "    print(f\"{i+1}. Score: {doc['score']:.3f}\")\n",
    "    print(f\"   {doc['document']['text'][:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RAGEvaluator()\n",
    "\n",
    "metrics = evaluator.evaluate_answer(\n",
    "    answer=result['answer'],\n",
    "    context=result['context'],\n",
    "    query=question\n",
    ")\n",
    "\n",
    "print(\"Evaluation Metrics:\\n\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Multiple Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get evaluation questions\n",
    "questions = get_all_questions()\n",
    "\n",
    "print(f\"Total questions: {len(questions)}\\n\")\n",
    "\n",
    "# Display questions\n",
    "for q in questions[:5]:\n",
    "    print(f\"{q['id']}. {q['question']}\")\n",
    "    print(f\"   Category: {q['category']} | Difficulty: {q['difficulty']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test first 3 questions\n",
    "results_list = []\n",
    "\n",
    "for q in questions[:3]:\n",
    "    print(f\"\\nProcessing Q{q['id']}: {q['question']}\")\n",
    "    \n",
    "    result = rag_system.query(\n",
    "        query=q['question'],\n",
    "        llm_name='phi3'\n",
    "    )\n",
    "    \n",
    "    results_list.append(result)\n",
    "    \n",
    "    print(f\"Answer: {result['answer'][:150]}...\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interactive Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query function\n",
    "def ask_question(question):\n",
    "    result = rag_system.query(\n",
    "        query=question,\n",
    "        llm_name='phi3',\n",
    "        return_context=True\n",
    "    )\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(f\"Q: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nA: {result['answer']}\\n\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Retrieved {result['num_retrieved']} documents\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluator.evaluate_answer(\n",
    "        answer=result['answer'],\n",
    "        context=result['context'],\n",
    "        query=question\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMetrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v:.3f}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Try it\n",
    "ask_question(\"What are the most popular gaming laptops?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have results from multiple questions\n",
    "if results_list:\n",
    "    # Extract metrics\n",
    "    lengths = []\n",
    "    for r in results_list:\n",
    "        lengths.append(len(r['answer'].split()))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(lengths)), lengths)\n",
    "    plt.xlabel('Question ID')\n",
    "    plt.ylabel('Answer Length (words)')\n",
    "    plt.title('Answer Lengths Across Questions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Load all three LLMs for comparison\n",
    "2. Run evaluation on all questions\n",
    "3. Compare model performance\n",
    "4. Generate final report\n",
    "\n",
    "Use the main.py script for full evaluation:\n",
    "```bash\n",
    "python main.py --build-index\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
