# ShopSmart RAG: E-commerce Intelligence System
## Air Conditioner Product Analysis with Local LLMs

A Retrieval-Augmented Generation (RAG) system that helps make smart shopping decisions by analyzing Amazon Air Conditioner products using three local Ollama models: **Phi-3**, **Llama3**, and **Gemma2**.

---

## ğŸ¯ Project Overview

This project implements an intelligent question-answering system that:

- **Analyzes 50 Air Conditioner products** from Amazon India
- **Combines structured data** (prices, ratings, brands) with **semantic search**
- **Compares 3 open-source LLMs** running locally via Ollama
- **Answers complex queries** about value, features, and recommendations
- **No cloud API costs** - everything runs on your machine!

---

## âœ¨ Key Features

### Value Reasoning
Not just "cheapest" but "best value for money"
- Combines price + specs + ratings
- Analyzes discount percentages
- Considers customer satisfaction

### Multi-Model Comparison
Compare answers from three different LLMs:
- **Phi-3** (3.8GB) - Fast and efficient
- **Llama3** (4.7GB) - Balanced performance
- **Gemma2** (5.4GB) - Latest Google model

### Domain-Specific Questions
15 evaluation questions across 4 categories:
1. **Structured Queries** - Price, rating filters
2. **Value Reasoning** - Best bang-for-buck analysis
3. **Temporal Analysis** - Review trends and consistency
4. **Combined** - Comprehensive recommendations

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Install and Start Ollama

**Windows:**
- Download from https://ollama.com/download/windows
- Install and it starts automatically

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama serve
```

**macOS:**
```bash
brew install ollama
ollama serve
```

### 3. Download Models

```bash
ollama pull phi3
ollama pull llama3
ollama pull gemma2
```

### 4. Run System Check

```bash
python quick_start.py
```

This will:
- âœ“ Check all installations
- âœ“ Verify models are available
- âœ“ Run a demo query

### 5. Build Vector Index

```bash
python main_ollama.py --build-index
```

### 6. Run Your First Query

```bash
python main_ollama.py --query "What are the best ACs under â‚¹35,000?"
```

---

## ğŸ“Š Dataset

**Source**: Amazon India - Air Conditioners
**Products**: 50
**Columns**:
- Product name
- Brand (LG, Samsung, Voltas, etc.)
- Ratings (1-5 stars)
- Number of ratings
- Discount price (â‚¹)
- Actual price (â‚¹)
- Category (Split AC, Window AC, Inverter, etc.)

**Price Range**: â‚¹26,490 - â‚¹52,990
**Average Rating**: 3.9 stars

---

## ğŸ’¡ Usage Examples

### Interactive Notebook
```bash
jupyter notebook notebooks/rag_ollama_demo.ipynb
```

### Single Query with All Models
```bash
python main_ollama.py \
  --query "Which 1.5 ton inverter AC offers best value?" \
  --models phi3 llama3 gemma2
```

### Run Evaluation (5 Questions)
```bash
python main_ollama.py --evaluate --num-questions 5
```

### Full Evaluation (All 15 Questions)
```bash
python main_ollama.py --evaluate
```

### Evaluate Specific Category
```bash
python main_ollama.py --evaluate-category value_reasoning
```

---

## ğŸ“ Sample Questions

1. "What are the top 5 rated air conditioners under â‚¹40,000?"
2. "Find the best value 1.5 ton AC considering specs and ratings"
3. "Which budget ACs (â‚¹25,000-â‚¹35,000) offer the best bang for buck?"
4. "Compare LG AC at â‚¹46,000 vs Voltas at â‚¹32,000 - which is better value?"
5. "Which brands maintain quality over time based on reviews?"

---

## ğŸ—ï¸ System Architecture

```
USER QUERY
    â†“
VECTOR STORE (FAISS)
    â†“ (Retrieve top-k products)
CONTEXT FORMATION
    â†“
LLM GENERATION (Ollama: Phi-3/Llama3/Gemma2)
    â†“
ANSWER with REASONING
```

### Components:

1. **Data Loader** - Loads and preprocesses CSV
2. **Vector Store** - FAISS with sentence-transformers
3. **Ollama Handler** - Manages local LLMs
4. **RAG System** - Combines retrieval + generation
5. **Evaluator** - Measures answer quality

---

## ğŸ“ Project Structure

```
rag/
â”œâ”€â”€ config.yaml                    # System configuration
â”œâ”€â”€ main_ollama.py                 # Main execution script
â”œâ”€â”€ quick_start.py                 # Setup verification
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ README_OLLAMA.md               # This file
â”œâ”€â”€ SETUP_GUIDE.md                 # Detailed setup instructions
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Air Conditioners.csv       # Product dataset
â”‚   â””â”€â”€ vector_index/              # FAISS index (auto-created)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ollama_handler.py          # Ollama LLM integration
â”‚   â”œâ”€â”€ data_loader.py             # Data processing
â”‚   â”œâ”€â”€ vector_store.py            # Vector database
â”‚   â”œâ”€â”€ rag_system.py              # RAG pipeline
â”‚   â”œâ”€â”€ evaluation.py              # Metrics
â”‚   â””â”€â”€ questions.py               # 15 eval questions
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ rag_ollama_demo.ipynb      # Interactive demo
â”‚
â””â”€â”€ outputs/                       # Generated results
    â”œâ”€â”€ evaluation_*.json
    â””â”€â”€ summary_*.txt
```

---

## ğŸ“ Academic Context

This project fulfills requirements for **DSCI 6004: Natural Language Processing** term project:

### Requirements Met:
âœ… RAG system development
âœ… Three free/open-source LLMs (Phi-3, Llama3, Gemma2)
âœ… 15+ domain-specific questions
âœ… Comparative evaluation across models
âœ… Analysis of performance, accuracy, and reasoning

### Deliverables:
- âœ… Working code with documentation
- âœ… Jupyter notebook for exploration
- âœ… Evaluation results and comparison
- âœ… Setup guide and README

---

## ğŸ“Š Evaluation Metrics

The system evaluates models on:

1. **Answer Relevancy** - How well answers address the question
2. **Faithfulness** - Accuracy to retrieved context
3. **Context Precision** - Quality of retrieval
4. **Response Length** - Conciseness vs completeness
5. **Response Time** - Generation speed
6. **Factual Accuracy** - Correctness of prices/specs/ratings

---

## ğŸ”§ Configuration

Key settings in `config.yaml`:

```yaml
ollama:
  base_url: "http://localhost:11434"
  timeout: 120

llms:
  phi3:
    model_name: "phi3"
    max_new_tokens: 512
    temperature: 0.7

rag:
  top_k: 3
  chunk_size: 512
```

---

## ğŸ› Troubleshooting

### Ollama not running
```bash
# Check status
ollama list

# Start server (Linux/macOS)
ollama serve
```

### Model not found
```bash
# Download model
ollama pull phi3
```

### Out of memory
- Use only Phi-3 (smallest)
- Reduce `max_new_tokens` in config
- Close other applications

### Slow responses
- Phi-3 is fastest - use for testing
- Reduce `top_k` to retrieve fewer docs
- Lower `max_new_tokens`

See [SETUP_GUIDE.md](SETUP_GUIDE.md) for detailed troubleshooting.

---

## ğŸ“ˆ Performance Comparison

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| Phi-3 | 3.8GB | âš¡âš¡âš¡ | â­â­ | Quick testing |
| Llama3 | 4.7GB | âš¡âš¡ | â­â­â­ | Balanced use |
| Gemma2 | 5.4GB | âš¡ | â­â­â­ | Best quality |

*(Actual results vary based on hardware)*

---

## ğŸš€ Getting Started (TL;DR)

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Install Ollama (visit ollama.com)

# 3. Download models
ollama pull phi3 && ollama pull llama3 && ollama pull gemma2

# 4. Verify setup
python quick_start.py

# 5. Build index
python main_ollama.py --build-index

# 6. Run query
python main_ollama.py --query "Best budget AC?"

# 7. Full evaluation
python main_ollama.py --evaluate
```

---

**Happy analyzing! ğŸ‰**

For detailed instructions, see [SETUP_GUIDE.md](SETUP_GUIDE.md)
